{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1c369c-8062-40c4-8b8d-6e791a507d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9b21ae-a8a8-4b00-b885-381cde8ad58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bded374-be0b-4c67-8c76-bdd66a049906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"/home/tom/two/envapi/my-env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5ae8c7-ee48-483a-9513-73fc9570deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31dda741-acee-492e-86aa-0dd6449914b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    PyPDFLoader(\"https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf\"),\n",
    "    PyPDFLoader(\"https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf\"),\n",
    "    PyPDFLoader(\"https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture02.pdf\"),\n",
    "    PyPDFLoader(\"https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture03.pdf\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d99218-48bb-48b7-9895-8906df7e5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c611a41-d6cb-4ca0-bd1f-ee47ecb4a179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e8071f-8617-4819-843d-8112c2f31cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='MachineLearning-Lecture01  \\nInstructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine \\nlearning class. So what I wanna do today is ju st spend a little time going over the logistics \\nof the class, and then we\\'ll start to  talk a bit about machine learning.  \\nBy way of introduction, my name\\'s  Andrew Ng and I\\'ll be instru ctor for this class. And so \\nI personally work in machine learning, and I\\' ve worked on it for about 15 years now, and \\nI actually think that machine learning is th e most exciting field of all the computer \\nsciences. So I\\'m actually always excited about  teaching this class. Sometimes I actually \\nthink that machine learning is not only the most exciting thin g in computer science, but \\nthe most exciting thing in all of human e ndeavor, so maybe a little bias there.  \\nI also want to introduce the TAs, who are all graduate students doing research in or \\nrelated to the machine learni ng and all aspects of machin e learning. Paul Baumstarck \\nworks in machine learning and computer vision.  Catie Chang is actually a neuroscientist \\nwho applies machine learning algorithms to try to understand the human brain. Tom Do \\nis another PhD student, works in computa tional biology and in sort of the basic \\nfundamentals of human learning. Zico Kolter is  the head TA — he\\'s head TA two years \\nin a row now — works in machine learning a nd applies them to a bunch of robots. And \\nDaniel Ramage is — I guess he\\'s not here  — Daniel applies l earning algorithms to \\nproblems in natural language processing.  \\nSo you\\'ll get to know the TAs and me much be tter throughout this quarter, but just from \\nthe sorts of things the TA\\'s do, I hope you can  already tell that machine learning is a \\nhighly interdisciplinary topic in which just the TAs find l earning algorithms to problems \\nin computer vision and biology and robots a nd language. And machine learning is one of \\nthose things that has and is having a large impact on many applications.  \\nSo just in my own daily work, I actually frequently end up talking to people like \\nhelicopter pilots to biologists to people in  computer systems or databases to economists \\nand sort of also an unending stream of  people from industry coming to Stanford \\ninterested in applying machine learni ng methods to their own problems.  \\nSo yeah, this is fun. A couple of weeks ago, a student actually forwar ded to me an article \\nin \"Computer World\" about the 12 IT skills th at employers can\\'t say no to. So it\\'s about \\nsort of the 12 most desirabl e skills in all of IT and all of information technology, and \\ntopping the list was actually machine lear ning. So I think this is a good time to be \\nlearning this stuff and learning algorithms and having a large impact on many segments \\nof science and industry.  \\nI\\'m actually curious about something. Learni ng algorithms is one of the things that \\ntouches many areas of science and industrie s, and I\\'m just kind of curious. How many \\npeople here are computer science majors, are in the computer science department? Okay. \\nAbout half of you. How many people are from  EE? Oh, okay, maybe about a fifth. How ', metadata={'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf', 'page': 0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ee43f8-4932-49d8-9e2f-01daaf98e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4588426-1516-4f57-be3b-b5dd882bf941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94b0cee7-be01-4aa2-abe8-50369c553d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a7cd47-ef40-45d0-be4e-9de8914dcfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "656a519f-f34b-4133-8253-930c85500cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "510bae00-4157-42f1-bbfc-eebdde6cd98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"MachineLearning-Lecture01  \\nInstructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine \\nlearning class. So what I wanna do today is ju st spend a little time going over the logistics \\nof the class, and then we'll start to  talk a bit about machine learning.  \\nBy way of introduction, my name's  Andrew Ng and I'll be instru ctor for this class. And so \\nI personally work in machine learning, and I' ve worked on it for about 15 years now, and \\nI actually think that machine learning is th e most exciting field of all the computer \\nsciences. So I'm actually always excited about  teaching this class. Sometimes I actually \\nthink that machine learning is not only the most exciting thin g in computer science, but \\nthe most exciting thing in all of human e ndeavor, so maybe a little bias there.  \\nI also want to introduce the TAs, who are all graduate students doing research in or \\nrelated to the machine learni ng and all aspects of machin e learning. Paul Baumstarck \\nworks in machine learning and computer vision.  Catie Chang is actually a neuroscientist \\nwho applies machine learning algorithms to try to understand the human brain. Tom Do \\nis another PhD student, works in computa tional biology and in sort of the basic \\nfundamentals of human learning. Zico Kolter is  the head TA — he's head TA two years \\nin a row now — works in machine learning a nd applies them to a bunch of robots. And \\nDaniel Ramage is — I guess he's not here  — Daniel applies l earning algorithms to\", metadata={'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf', 'page': 0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aa643c6-dfe9-42c9-ac2a-f16429fd2de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Student: YI is [inaudible].  \\nInstructor (Andrew Ng) :Oh, yes. Thanks. So having maximized this function – well, it \\nturns out we can actually apply the same gradient descent algo rithm that we learned. That \\nwas the first algorithm we used to minimize the quadratic function. And you remember, when we talked about least squares, the first algorithm we used to minimize the quadratic \\nerror function was great in descent. So can actually use exactly the same algorithm to \\nmaximize the log likelihood. And you remember, that algorithm was just repeatedly take \\nthe value of theta and you repla ce it with the previous value of theta plus a learning rate \\nalpha times the gradient of the cos functi on. The log likelihood w ill respect the theta. \\nOkay? One small change is that because pr eviously we were trying to minimize the \\nquadratic error term. Today we’re trying to maximize rather than minimize. So rather \\nthan having a minus sign we have a plus sign. So this is just great in ascents, but for the \\nmaximization rather than the minimization. So we actually call this gradient ascent and \\nit’s really the same algorithm.  \\nSo to figure out what this grad ient – so in order to derive gradient descent, what you need \\nto do is compute the partial de rivatives of your objective func tion with respect to each of \\nyour parameters theta I, right? It turns out  that if you actually compute this partial', metadata={'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture03.pdf', 'page': 13})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bce76ef9-b4b6-4891-9402-a76fc2a42229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3131"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9632e434-a280-4621-99b3-b7a35b3fcd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1499"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e159782-db5d-4659-8ae4-2e8965eaf96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1419"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits[99].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d4644b4-8daa-4d20-bb9c-dc3b74ae59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7338e9d-e073-46ed-b8b1-a3f9e6c8c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7193990c-1b39-4650-9246-195aa1262728",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'I like dogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9db47450-0fd0-435c-90a9-a7e1c6f78bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2='I like canines'\n",
    "sentence3='the weather is ugly outside'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b57997f-7df7-49ad-aa0d-644742cb6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embeddings.embed_query(sentence1)\n",
    "embedding2 = embeddings.embed_query(sentence2)\n",
    "embedding3 = embeddings.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fca50039-5a35-44c5-8608-cf80e152e4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.02084651596240501,\n",
       " 0.0021781544272615914,\n",
       " -0.025506325149863285,\n",
       " -0.023384885939275617,\n",
       " -0.029969932609882906,\n",
       " 0.021570012044226164,\n",
       " -0.011300037652115347,\n",
       " -0.006027095505274193,\n",
       " 0.011943827422448821,\n",
       " -0.02038053429860112,\n",
       " 0.006879350193087847,\n",
       " 0.03087736862608505,\n",
       " -0.0010185668976903936,\n",
       " -0.004506526064027877,\n",
       " 0.0031944220059230296,\n",
       " 0.01133682582525606,\n",
       " 0.037303000811071824,\n",
       " -0.0017688882129622693,\n",
       " 0.006183444542630294,\n",
       " -0.00016276761069802616,\n",
       " -0.007700948535612195,\n",
       " 0.005846221484818904,\n",
       " 0.010036984491531451,\n",
       " -0.03521835070494745,\n",
       " -0.008117878836233843,\n",
       " 0.011379744894925605,\n",
       " 0.009938882696489546,\n",
       " -0.002812746921479243,\n",
       " -0.027664553464914244,\n",
       " -0.018271358743716267,\n",
       " 0.0372784772249565,\n",
       " -0.0017872821831173042,\n",
       " -0.01803836791181432,\n",
       " -0.021324759419266558,\n",
       " 0.00508900267812145,\n",
       " -0.027124995454828927,\n",
       " 0.0057971710529592406,\n",
       " -0.004500395167499047,\n",
       " -0.001894580439367776,\n",
       " -0.014102054806177202,\n",
       " -0.012391414534021063,\n",
       " 0.02210957005431148,\n",
       " 0.008479626877144422,\n",
       " -0.0242800601624201,\n",
       " -0.0015290000060500684,\n",
       " 0.011907039249308107,\n",
       " 0.00018844254434999676,\n",
       " -0.020723888718602628,\n",
       " -0.024782829068042127,\n",
       " -0.00587074693357938,\n",
       " 0.004678203274028631,\n",
       " -0.008124009732762673,\n",
       " -0.01646874850436963,\n",
       " -0.01309651699493315,\n",
       " -0.007749998967471858,\n",
       " 0.00509513357465028,\n",
       " -0.0028434034995991936,\n",
       " 0.00949742741276871,\n",
       " -0.0015274671655025388,\n",
       " -0.012379151809640826,\n",
       " 0.007964595247142157,\n",
       " 0.004969441348244773,\n",
       " -0.01105478502715574,\n",
       " 0.019485361938101787,\n",
       " -0.0028142796456114504,\n",
       " -0.005147249920435647,\n",
       " 0.027640028016153766,\n",
       " 0.022955693379935017,\n",
       " 0.005827827398248547,\n",
       " 0.01072369333153447,\n",
       " 0.005165644007006004,\n",
       " -0.0180628933605748,\n",
       " -0.017057355549330747,\n",
       " 0.02206051915679053,\n",
       " 0.011907039249308107,\n",
       " 0.009313487478387719,\n",
       " 0.0006748292966558325,\n",
       " -0.0006066182997443863,\n",
       " 0.008332475115904143,\n",
       " -0.012924839784932396,\n",
       " 0.01643195939990634,\n",
       " -0.03715584811850897,\n",
       " -0.008283424218383191,\n",
       " 0.0020141413035831635,\n",
       " 0.011563684829306597,\n",
       " 0.005355715303577116,\n",
       " -0.016983779203049318,\n",
       " 0.007357594115610685,\n",
       " -0.00126228726417908,\n",
       " -0.016971517409991657,\n",
       " 0.009258305684337937,\n",
       " 0.03041138882492632,\n",
       " -0.001269184988435303,\n",
       " 0.03568433236875134,\n",
       " -0.01668947568056876,\n",
       " -0.005392503011056541,\n",
       " -0.004573971048119185,\n",
       " 0.012655061711212315,\n",
       " -0.0037033222737351745,\n",
       " -0.02977373088244425,\n",
       " -0.0006468551603902283,\n",
       " 0.006952926073707986,\n",
       " -0.03215268637369434,\n",
       " -0.004436016097333441,\n",
       " -0.00403134852109203,\n",
       " -0.003939378553901534,\n",
       " -0.004901996829814753,\n",
       " -0.013562497727414463,\n",
       " 0.02024564526174108,\n",
       " 0.003304786059683883,\n",
       " 0.006425632184986773,\n",
       " 0.030018983507403858,\n",
       " 0.016946991961231183,\n",
       " -0.05111075209476847,\n",
       " -0.01234849546435152,\n",
       " -0.013537973209976564,\n",
       " 0.0068302997612281835,\n",
       " -0.018896753030495518,\n",
       " -0.011165148615255305,\n",
       " 0.0014408621984360575,\n",
       " 0.013660599522456367,\n",
       " 0.005395568924982245,\n",
       " 0.04029509065868866,\n",
       " -0.004236747990307796,\n",
       " 0.009656841898389227,\n",
       " 0.008283424218383191,\n",
       " -0.010417127084673674,\n",
       " -0.006214101353580889,\n",
       " -0.006768986604988282,\n",
       " 0.01269184988435303,\n",
       " 0.021913366464227674,\n",
       " 0.032741293418655455,\n",
       " 0.006401106736226297,\n",
       " 0.014923652683040262,\n",
       " -0.02301700607051363,\n",
       " 0.011447190344678202,\n",
       " -0.0012783819153051592,\n",
       " 0.01879865309809877,\n",
       " -0.014702925506841134,\n",
       " -0.017744064389333764,\n",
       " -0.007891019832183306,\n",
       " 0.03249603893105069,\n",
       " -0.011612735726827549,\n",
       " -0.03146597753369132,\n",
       " 0.02370371491051665,\n",
       " 0.04735837892351234,\n",
       " 0.02361787677117756,\n",
       " -0.014236943843037244,\n",
       " 0.014457671950558951,\n",
       " 0.003939378553901534,\n",
       " 0.013145567892454102,\n",
       " -0.01317009334121458,\n",
       " 0.028719142173679248,\n",
       " -0.000766032843572564,\n",
       " -0.02125118307298513,\n",
       " -0.005971913711224411,\n",
       " -0.009595529207810614,\n",
       " 0.014886865441222126,\n",
       " 0.010668511537484687,\n",
       " 0.00576344879374423,\n",
       " 0.021128557691827907,\n",
       " -0.018835440339916906,\n",
       " -0.0027866887485865593,\n",
       " -0.02038053429860112,\n",
       " 0.011177411339635544,\n",
       " 0.044022936518539155,\n",
       " 0.026364712131189634,\n",
       " 0.01105478502715574,\n",
       " -0.008859769470286645,\n",
       " -0.0003527429664917221,\n",
       " 0.010202529873680798,\n",
       " 0.038234964138679606,\n",
       " -0.02253876261365208,\n",
       " 0.027394775391194164,\n",
       " -0.0335506276398157,\n",
       " 0.0072288359752794755,\n",
       " -0.00916633571714744,\n",
       " 0.00064877118197081,\n",
       " -0.050718348639891164,\n",
       " -0.0008606851944097457,\n",
       " -0.008099484284002197,\n",
       " 0.013832276732457122,\n",
       " 0.03234888623848784,\n",
       " 0.014249206567417481,\n",
       " 0.00042191203240112034,\n",
       " 0.0007139165559948579,\n",
       " 0.008498021429376067,\n",
       " -0.015230219861223636,\n",
       " 0.0036328120742100947,\n",
       " 0.0055580488588671765,\n",
       " 0.0012438931776087231,\n",
       " 0.009184729338056507,\n",
       " 0.0004977872029463848,\n",
       " -0.006627965740276834,\n",
       " -0.6765062235163382,\n",
       " -0.009184729338056507,\n",
       " 0.02110403224306743,\n",
       " 0.001215535802102406,\n",
       " 0.029871830814841,\n",
       " 0.03735205170859278,\n",
       " -0.017143193688669834,\n",
       " 0.0011764486591787028,\n",
       " -0.001443927763115795,\n",
       " 0.004598496031218373,\n",
       " -0.008179191526812455,\n",
       " 0.011097704096825286,\n",
       " 0.007817443485901878,\n",
       " 0.020172070778104807,\n",
       " 0.022918904275471726,\n",
       " -0.0160150286336234,\n",
       " 0.003079459546934909,\n",
       " -0.024267796506717287,\n",
       " -0.016186706774946734,\n",
       " -0.0017029764186644565,\n",
       " -0.01825909508801345,\n",
       " 0.02497903079548078,\n",
       " 0.004258207525142567,\n",
       " -0.014077529357416727,\n",
       " 0.013660599522456367,\n",
       " -0.00835086873681321,\n",
       " 0.0023881523017046232,\n",
       " -0.022354822679271087,\n",
       " -0.003642008884664629,\n",
       " 0.015929190494284314,\n",
       " -0.02172942652984668,\n",
       " 0.006934532452798918,\n",
       " -0.008657435914996583,\n",
       " 0.002006477217260837,\n",
       " 0.054691448987346424,\n",
       " 0.010355813462772484,\n",
       " 0.004880537294979981,\n",
       " 0.034727841729737925,\n",
       " 0.017216770034951263,\n",
       " 0.03367325674626324,\n",
       " -0.0029537673205294024,\n",
       " -0.006548258497466575,\n",
       " 0.004411490648572965,\n",
       " 0.010165742631862661,\n",
       " -0.041006323084807,\n",
       " -0.009920489075580479,\n",
       " 0.014641611884939943,\n",
       " 0.02176621563430997,\n",
       " 0.021435123007366122,\n",
       " -0.006781248863707232,\n",
       " -0.017952529772475235,\n",
       " 0.005248417163741966,\n",
       " -0.0023007809725720386,\n",
       " 0.003930181277785712,\n",
       " 0.01031289439310294,\n",
       " -0.003648140246854748,\n",
       " 0.02584967956986479,\n",
       " -0.03200553368113149,\n",
       " 0.01677531381990785,\n",
       " -0.0020846515031082433,\n",
       " 0.00922764933904863,\n",
       " -0.008522545946813965,\n",
       " -0.008608385017475632,\n",
       " -0.000836159920272253,\n",
       " -0.028817242106075995,\n",
       " 0.020601263337445403,\n",
       " -0.020883303204223144,\n",
       " 0.0002854899803705667,\n",
       " 0.006106802748084451,\n",
       " -0.02062578878620588,\n",
       " 0.013881327629978074,\n",
       " 0.014053004839978829,\n",
       " -0.004681269187954335,\n",
       " 0.011716968418398284,\n",
       " -0.0009833117979278537,\n",
       " 0.012949365233692873,\n",
       " 0.0035193823393541816,\n",
       " -0.012189080978731004,\n",
       " -0.015659412420564234,\n",
       " 0.0016631227972593275,\n",
       " 0.0060884091271753826,\n",
       " 0.0009196992644513357,\n",
       " -0.021226657624224654,\n",
       " -0.01235462636088035,\n",
       " 0.02125118307298513,\n",
       " -0.00012990752335929803,\n",
       " -0.0077316048809015015,\n",
       " 0.006480813979036555,\n",
       " 0.0006572017467745626,\n",
       " -0.004491197891383224,\n",
       " 0.021067243138604138,\n",
       " 0.03860284400744159,\n",
       " -0.005803302415149359,\n",
       " -0.01925237110619984,\n",
       " -0.018528873161733534,\n",
       " 0.007314674580279852,\n",
       " -0.009693630071529942,\n",
       " 0.012740899850551404,\n",
       " -0.0018700551070226221,\n",
       " -0.02553085059862376,\n",
       " 0.0074802204280904885,\n",
       " 0.0007583686408349152,\n",
       " 0.0017290345915571402,\n",
       " 0.007326937304660091,\n",
       " 0.009411589273429622,\n",
       " -0.0004019852216985557,\n",
       " -0.012048060114019555,\n",
       " 0.013942640320556686,\n",
       " -0.0023896852586674754,\n",
       " -0.019178794759918415,\n",
       " 0.008743274054335672,\n",
       " 0.0032066847303032675,\n",
       " 0.0010798801703456172,\n",
       " -0.0019099087284277512,\n",
       " 0.002835739413276867,\n",
       " -0.020061705327360086,\n",
       " -0.0010438585921016513,\n",
       " 0.012753162574931642,\n",
       " 0.014236943843037244,\n",
       " -0.027370249942433686,\n",
       " -0.007087815576229316,\n",
       " 0.005699069723578625,\n",
       " 0.013084254270552913,\n",
       " -0.011410402171537489,\n",
       " -0.008265030597474122,\n",
       " 0.00780518076152164,\n",
       " -0.030852843177324576,\n",
       " -0.024721516377463514,\n",
       " 0.021398335765547987,\n",
       " -0.02210957005431148,\n",
       " -0.006848693847798541,\n",
       " -0.009270568408718174,\n",
       " 0.014089792081796965,\n",
       " -0.0071613914568494546,\n",
       " -0.005987241883869063,\n",
       " 0.011520765759637053,\n",
       " 0.028719142173679248,\n",
       " 0.01276542529931188,\n",
       " 0.019485361938101787,\n",
       " -0.005337321217006759,\n",
       " -0.013820014008076883,\n",
       " 0.012875789818734022,\n",
       " -0.022268984539931997,\n",
       " -0.0002937289546577933,\n",
       " 0.0059381914520094,\n",
       " -0.004868274570599743,\n",
       " -0.0004682801970294204,\n",
       " 0.0005207796947440089,\n",
       " 0.010527490672773239,\n",
       " 0.005815564673868309,\n",
       " -0.008847506745906406,\n",
       " -0.015021754478082166,\n",
       " -0.0035469734692097172,\n",
       " -0.0027499008082764896,\n",
       " 0.005699069723578625,\n",
       " 0.007768393054042215,\n",
       " -0.011698574797489215,\n",
       " 0.0023237734643696624,\n",
       " -0.00492039091638511,\n",
       " -0.012875789818734022,\n",
       " 0.004503460615763462,\n",
       " -0.002867928715529025,\n",
       " -0.004770173241219128,\n",
       " -0.0022118766864766015,\n",
       " -0.005334255768742344,\n",
       " -0.021042719552488817,\n",
       " -0.0018393985289026713,\n",
       " 0.020037181741244765,\n",
       " -0.005073674039815508,\n",
       " -0.02913607107731703,\n",
       " 0.005512064340933217,\n",
       " 0.002046330838665966,\n",
       " -0.006744461156227806,\n",
       " -0.012777688023692118,\n",
       " 0.0007349929680042392,\n",
       " 0.00024467831907131615,\n",
       " -0.0076764230868517185,\n",
       " -0.030239710683602986,\n",
       " -0.006891612917468085,\n",
       " -0.0020294697090584607,\n",
       " 0.012606010813691363,\n",
       " 0.010067640836820758,\n",
       " -0.008731011329955435,\n",
       " -0.0032465383517083965,\n",
       " 0.035561703262303804,\n",
       " 0.013501185036835851,\n",
       " 0.033943034819983314,\n",
       " 0.04490584894862598,\n",
       " -0.006542127135276457,\n",
       " -0.007216573250899237,\n",
       " 0.007982989799373803,\n",
       " 0.003654271609044867,\n",
       " -0.0017673553724147396,\n",
       " -0.0024893193121802984,\n",
       " 0.00475177962031006,\n",
       " -0.008136272457142912,\n",
       " 0.012133898253358644,\n",
       " 0.020221119812980602,\n",
       " 0.020797465064884058,\n",
       " 0.010306762565251532,\n",
       " -0.006621834378086715,\n",
       " -0.01133069492872723,\n",
       " 0.006768986604988282,\n",
       " -0.016542322988005904,\n",
       " 0.017265820932472215,\n",
       " -0.03867641849107786,\n",
       " -0.0010308296220706318,\n",
       " -0.010619460639963735,\n",
       " 0.019043905723058373,\n",
       " 0.025776103223583365,\n",
       " -0.006983582884658581,\n",
       " 0.006358187200895464,\n",
       " -0.01085245147186568,\n",
       " -0.012777688023692118,\n",
       " 0.012268788221541262,\n",
       " 0.03786708426991762,\n",
       " 0.002397349344989802,\n",
       " 0.008780062227476387,\n",
       " -0.004613824669524315,\n",
       " -0.001531299208663702,\n",
       " 0.002636471073420577,\n",
       " 0.0035285793826393603,\n",
       " 0.00032821766325039884,\n",
       " -0.024733778170521175,\n",
       " -0.00404361077981098,\n",
       " 0.0227962788943145,\n",
       " 0.007835837106810947,\n",
       " 0.023605613115474745,\n",
       " -0.013022941579974301,\n",
       " 0.0009610857264039947,\n",
       " -0.03938765464248651,\n",
       " 0.008142404284994319,\n",
       " 0.002455597052965288,\n",
       " 0.015303795276182485,\n",
       " -0.0021306364867034913,\n",
       " -0.016530061194948244,\n",
       " 0.02869461672491877,\n",
       " -0.019742876356119054,\n",
       " 0.026119457643584875,\n",
       " 0.0017351659537472592,\n",
       " 0.007418907271850587,\n",
       " 0.013047467028734777,\n",
       " 0.00801977704119194,\n",
       " -0.01263053626245184,\n",
       " 0.019399521936117544,\n",
       " 0.029626578189881396,\n",
       " -0.0037768981543553136,\n",
       " 0.0034979228045194095,\n",
       " -0.003160699746708019,\n",
       " 9.848445948196378e-05,\n",
       " -0.015573574281225144,\n",
       " -0.014236943843037244,\n",
       " 0.004120252574356823,\n",
       " 0.017278082725529876,\n",
       " 0.0019390325824154943,\n",
       " -0.03565980505734571,\n",
       " 0.0050798054020056264,\n",
       " -0.0013718847230431853,\n",
       " 0.030313287029884416,\n",
       " 0.03330537687750125,\n",
       " 0.01989002904868191,\n",
       " 0.005748120155438289,\n",
       " -0.006134394110770631,\n",
       " -0.02116534493364604,\n",
       " 0.01703283010057027,\n",
       " 0.008032039765572178,\n",
       " 0.0001157288210542242,\n",
       " -0.020098494431823377,\n",
       " -0.005052214504980735,\n",
       " -0.00773773624309162,\n",
       " -0.01547547248618324,\n",
       " -0.009006921231526924,\n",
       " 0.008528677774665374,\n",
       " -0.01581882690618475,\n",
       " 0.004123318488282527,\n",
       " 0.0011565218484761383,\n",
       " -0.012477252673360154,\n",
       " 0.020748414167363106,\n",
       " -0.004350177492333064,\n",
       " 0.03745015350363468,\n",
       " -0.0057174638101489825,\n",
       " -0.01046004615434322,\n",
       " 0.005043017694526202,\n",
       " 0.03874999297471413,\n",
       " 0.012777688023692118,\n",
       " -0.015708463318085186,\n",
       " -0.0242800601624201,\n",
       " 0.00011592042612266542,\n",
       " -0.008773930399624978,\n",
       " 0.03781803337239667,\n",
       " -0.00027514336488240195,\n",
       " 0.009926620903431887,\n",
       " 0.0010737488081554983,\n",
       " -0.003654271609044867,\n",
       " -0.014592560987418991,\n",
       " 0.00827729332185436,\n",
       " 0.02053995064686679,\n",
       " -0.0180628933605748,\n",
       " -0.00016755770830522608,\n",
       " -0.01676305202685019,\n",
       " 0.02275948978985121,\n",
       " 0.011735362039307352,\n",
       " -0.022698177099272597,\n",
       " -0.009129547544006725,\n",
       " 0.0076641603624714805,\n",
       " -0.01079113784996449,\n",
       " -0.018774127649338293,\n",
       " -0.00638271264965594,\n",
       " -0.019473098282398973,\n",
       " -0.007694817173422076,\n",
       " 0.0047885673277894844,\n",
       " -0.003412084199519032,\n",
       " -0.011655654796497094,\n",
       " -0.007848099831191184,\n",
       " -0.02028243436620437,\n",
       " 0.013991691218077638,\n",
       " -0.02049089974934584,\n",
       " -0.0024050134313121285,\n",
       " 0.019399521936117544,\n",
       " 0.011122229545585761,\n",
       " -0.007860362555571423,\n",
       " -0.011379744894925605,\n",
       " -0.016530061194948244,\n",
       " -0.0004786268125175851,\n",
       " 0.10555695031980045,\n",
       " 0.0074924831524707265,\n",
       " 0.014555773745600855,\n",
       " 0.013403083241793946,\n",
       " 0.026511862961107337,\n",
       " 0.01038646980806179,\n",
       " -0.02621755943862678,\n",
       " -0.00834473784028438,\n",
       " 0.0045770364963836005,\n",
       " 0.0076764230868517185,\n",
       " -0.02125118307298513,\n",
       " -0.021655852046210407,\n",
       " 0.014813289094940698,\n",
       " -0.0024234075178824854,\n",
       " 0.018958067583719287,\n",
       " -0.0002086567699311808,\n",
       " -0.002903183931706887,\n",
       " -0.01324366875617343,\n",
       " -0.0018869162366301272,\n",
       " -0.012379151809640826,\n",
       " 0.0026686606085033794,\n",
       " -0.012060322838399792,\n",
       " 0.008326344219375312,\n",
       " 0.029749205433683777,\n",
       " 0.022379348128031565,\n",
       " 0.004923456830310814,\n",
       " 0.007308543218089734,\n",
       " 0.009258305684337937,\n",
       " -0.001905310206785162,\n",
       " -0.011392007619305842,\n",
       " -0.013059728821792438,\n",
       " 0.015242481654281296,\n",
       " -0.0037830295165454325,\n",
       " -0.0118763829040188,\n",
       " -0.015230219861223636,\n",
       " -0.00027284410406110734,\n",
       " -0.008534808671194204,\n",
       " 0.03301107149237554,\n",
       " 0.02150869935364755,\n",
       " -0.0037155849981154124,\n",
       " 0.03166217739848482,\n",
       " 0.02083425416934735,\n",
       " 0.006995845609038819,\n",
       " -0.015769776008663798,\n",
       " 0.011079310475916216,\n",
       " -0.020736152374305445,\n",
       " -0.024427210992337803,\n",
       " 0.027738129811195673,\n",
       " -0.007749998967471858,\n",
       " -0.015009491753701929,\n",
       " 0.02999445805864338,\n",
       " 0.007136866008088979,\n",
       " -0.02271044075497541,\n",
       " -0.021349284868027035,\n",
       " -0.0006660155508190281,\n",
       " 0.0012891117991378675,\n",
       " -0.02656091385862829,\n",
       " -0.010343550738392247,\n",
       " -0.021079506794306955,\n",
       " -0.012814476196832831,\n",
       " -0.027689078913674718,\n",
       " -0.03139239932476474,\n",
       " -0.012373020913111996,\n",
       " -0.018344935089997697,\n",
       " 0.0019206386122604595,\n",
       " -0.055525310519912295,\n",
       " -0.024991294451183598,\n",
       " 0.0007560694382212817,\n",
       " -0.020172070778104807,\n",
       " -0.005650018826057673,\n",
       " -0.0158678778037057,\n",
       " -0.016248019465525346,\n",
       " -0.017535599006192296,\n",
       " -0.006781248863707232,\n",
       " 0.01933820924553893,\n",
       " 0.005043017694526202,\n",
       " 0.01513211806618173,\n",
       " -0.008939476713096903,\n",
       " 0.014433146501798473,\n",
       " 0.024831879965563082,\n",
       " -0.006358187200895464,\n",
       " -0.002544501339060725,\n",
       " 0.0171064064468517,\n",
       " 0.00481922413874008,\n",
       " -0.012655061711212315,\n",
       " 0.008032039765572178,\n",
       " -0.005269876698576739,\n",
       " 0.0020570606060833522,\n",
       " 0.0031760281521833167,\n",
       " 0.003596024133900025,\n",
       " 0.014003953942457877,\n",
       " -0.007841968934662354,\n",
       " 0.019056167516116034,\n",
       " 0.006125196834654808,\n",
       " 0.01771953894057329,\n",
       " 0.011624998451207787,\n",
       " -0.001029296781523102,\n",
       " 0.02272270254803307,\n",
       " -0.014457671950558951,\n",
       " 0.0019865504065582724,\n",
       " 0.03026423613236346,\n",
       " -0.0003205535187204112,\n",
       " -0.02309058241679506,\n",
       " -0.002676324694825706,\n",
       " 0.02626661033614773,\n",
       " 0.013844539456837359,\n",
       " 0.016603637541229673,\n",
       " 0.006370449925275702,\n",
       " -0.01824683329495579,\n",
       " 0.0021781544272615914,\n",
       " 0.02580062867234384,\n",
       " -0.03539002698362562,\n",
       " 0.033256325979980295,\n",
       " -0.0007721641475550219,\n",
       " 0.0118763829040188,\n",
       " 0.024562101891842998,\n",
       " -0.01451898557246014,\n",
       " 0.00998180269748167,\n",
       " -0.0038504742678060976,\n",
       " -0.010276106219962226,\n",
       " -0.005496735702627276,\n",
       " -0.006762855242798163,\n",
       " 0.043507903957214315,\n",
       " 0.026438288477471063,\n",
       " -0.01769501349181281,\n",
       " -0.024942243553662646,\n",
       " 0.012924839784932396,\n",
       " -0.009111153923097658,\n",
       " -0.011379744894925605,\n",
       " -0.01181506928211761,\n",
       " -0.004098793039522051,\n",
       " 0.0316376538123695,\n",
       " -0.02176621563430997,\n",
       " -0.015708463318085186,\n",
       " -0.030288761581123938,\n",
       " 0.01471518823122137,\n",
       " 0.012366889085260587,\n",
       " 0.014469934674939188,\n",
       " -0.016002766840565744,\n",
       " -0.0057971710529592406,\n",
       " -0.012428202707161778,\n",
       " 0.0010530556353868297,\n",
       " -0.005886075106224033,\n",
       " -0.013010678855594062,\n",
       " -0.01608860497990483,\n",
       " -0.032962020594854584,\n",
       " -0.01812420605115341,\n",
       " -0.016395172158088205,\n",
       " -0.00475177962031006,\n",
       " 0.032446988033529744,\n",
       " -0.02945490191120322,\n",
       " 0.009258305684337937,\n",
       " -0.008706485881194957,\n",
       " 0.006177313180440175,\n",
       " -0.001099040618982078,\n",
       " -0.020319221608022506,\n",
       " 0.005092068126385864,\n",
       " 0.011342957653107468,\n",
       " 0.016983779203049318,\n",
       " 0.037106797220988016,\n",
       " 0.01673852657808971,\n",
       " -0.000338947488875446,\n",
       " 0.016726262922386898,\n",
       " 0.00678738022589735,\n",
       " -0.00583702467436437,\n",
       " -0.012471121776831323,\n",
       " -0.0008484225282371688,\n",
       " 4.8332112941843734e-05,\n",
       " -0.008737143157806842,\n",
       " 0.018896753030495518,\n",
       " -0.003062598417327404,\n",
       " -0.003605220944354559,\n",
       " 0.030632116001125448,\n",
       " 0.017854427977433328,\n",
       " -0.028792718519960677,\n",
       " 0.005361846665767235,\n",
       " 0.011974483767738126,\n",
       " -0.013623811349315652,\n",
       " -0.014751975473039507,\n",
       " -0.005196300817956599,\n",
       " 0.015683937869324708,\n",
       " 0.000800904762302052,\n",
       " 0.01676305202685019,\n",
       " 0.010668511537484687,\n",
       " 0.01711866823990936,\n",
       " 0.008455101428383944,\n",
       " 0.0185901877149573,\n",
       " -0.012403677258401302,\n",
       " 0.01928915834801798,\n",
       " -0.01092602688682453,\n",
       " 0.020135281673641515,\n",
       " -0.022685915306214936,\n",
       " -0.009178598441527677,\n",
       " -0.0002709280824805257,\n",
       " -0.004242879352497914,\n",
       " -0.007075552851849077,\n",
       " -0.014126580254937678,\n",
       " 0.0039547067265461875,\n",
       " -0.0025015820365605363,\n",
       " 0.012998416131213825,\n",
       " 0.02538369976870606,\n",
       " 0.024120645676799585,\n",
       " -0.014335045638079148,\n",
       " 0.012201343703111241,\n",
       " -0.008675829535905651,\n",
       " 0.020858779618107826,\n",
       " -0.011943827422448821,\n",
       " -0.03038686337616584,\n",
       " 0.014776500921799983,\n",
       " -0.029013445696159806,\n",
       " -0.01378322583493617,\n",
       " -0.030534014206083544,\n",
       " -0.0342128128937034,\n",
       " -0.023200946004894623,\n",
       " 0.005573377497173119,\n",
       " 0.01744976086685321,\n",
       " -0.02382634215431903,\n",
       " 0.02030695981496485,\n",
       " 0.013893589423035735,\n",
       " -0.009098891198717419,\n",
       " -0.0036266807120199757,\n",
       " 0.02055221243992445,\n",
       " 0.023446198629854226,\n",
       " 0.0018210044423323144,\n",
       " 0.00658504667060729,\n",
       " 0.028400313202438215,\n",
       " 0.002332970507654841,\n",
       " -0.029969932609882906,\n",
       " 0.02913607107731703,\n",
       " -0.0029890225367072646,\n",
       " -0.00195129519038041,\n",
       " 0.0018363328478076119,\n",
       " 0.027762655259956148,\n",
       " -0.004058939418116921,\n",
       " -0.009926620903431887,\n",
       " 0.01296162795807311,\n",
       " -0.005441553908577493,\n",
       " -0.009534215585909426,\n",
       " -0.020392797954303935,\n",
       " 0.026511862961107337,\n",
       " -0.00481309277654996,\n",
       " 0.01953441283562274,\n",
       " -0.012520172674352275,\n",
       " -0.015573574281225144,\n",
       " -0.010527490672773239,\n",
       " 0.006756723880608044,\n",
       " -0.021054981345546477,\n",
       " 0.01406526663303649,\n",
       " -0.029307749218640364,\n",
       " 0.004255141611216864,\n",
       " -0.016284808569988637,\n",
       " 0.005171775369196123,\n",
       " -0.02111629403612509,\n",
       " 0.03835758951983683,\n",
       " -0.022918904275471726,\n",
       " 0.012066453734928623,\n",
       " -0.004101858487786466,\n",
       " -0.017596911696770908,\n",
       " 0.0019374997418679645,\n",
       " -0.021692639288028545,\n",
       " 0.0022823868860016813,\n",
       " 0.019828716358103297,\n",
       " -0.031196197597326086,\n",
       " -0.014751975473039507,\n",
       " 0.006634097102466953,\n",
       " -0.011802806557737373,\n",
       " -0.001386446650037057,\n",
       " 0.005141118558245528,\n",
       " -0.002544501339060725,\n",
       " 0.026021357711188124,\n",
       " -0.019129743862397464,\n",
       " -0.0016523930298419415,\n",
       " -0.019914554497442387,\n",
       " 0.006119065472464689,\n",
       " -0.00835086873681321,\n",
       " 0.010735956055914708,\n",
       " -0.00786649438342283,\n",
       " -0.016064079531144356,\n",
       " 0.0020310024331906683,\n",
       " -0.011385876722777011,\n",
       " 0.021888842878112352,\n",
       " -0.001974287682178034,\n",
       " -0.005625493842958486,\n",
       " -0.002449465690775169,\n",
       " -0.018958067583719287,\n",
       " 0.0086267786383847,\n",
       " -0.02336036049051514,\n",
       " -0.0026042817711684186,\n",
       " -0.0028771257588142037,\n",
       " 0.000455634320719961,\n",
       " -0.0006407237982001094,\n",
       " 0.001554291700461326,\n",
       " -0.008914951264336427,\n",
       " -0.002092315822261214,\n",
       " 0.026438288477471063,\n",
       " -0.009534215585909426,\n",
       " 0.033207275082459346,\n",
       " 0.0468433463621875,\n",
       " -0.033967558406098636,\n",
       " 0.01860244950801496,\n",
       " -0.006419500822796654,\n",
       " 0.0073330686668502095,\n",
       " -0.037695407991239445,\n",
       " 0.0057052010857687446,\n",
       " 0.0005326591216758728,\n",
       " 5.3888627184829716e-05,\n",
       " -0.0009166335833562762,\n",
       " 0.02790980608987385,\n",
       " -0.011079310475916216,\n",
       " -0.0025874206415609133,\n",
       " 0.009656841898389227,\n",
       " 0.015929190494284314,\n",
       " -0.005125790385600875,\n",
       " -0.016223494016764872,\n",
       " 0.0008913418307373575,\n",
       " 0.007903282556563545,\n",
       " 0.0308283177285641,\n",
       " -0.030926419523606006,\n",
       " -0.026585439307388763,\n",
       " 0.02118987038240652,\n",
       " 0.009270568408718174,\n",
       " -0.0026395367545156362,\n",
       " 0.009583266483430377,\n",
       " -0.017216770034951263,\n",
       " 0.0012998415665552536,\n",
       " 0.005981110521678944,\n",
       " 0.019877765392979092,\n",
       " 0.008681960432434481,\n",
       " -0.02053995064686679,\n",
       " -0.025162970729861775,\n",
       " -0.03860284400744159,\n",
       " 0.021275708521745606,\n",
       " -0.012446596328070847,\n",
       " -0.01621123222370721,\n",
       " -0.006462419892466198,\n",
       " -0.01242207087931037,\n",
       " -0.022551026269354894,\n",
       " 0.013954903044936925,\n",
       " 0.005171775369196123,\n",
       " -0.005475276167792503,\n",
       " 0.020061705327360086,\n",
       " 0.029283223769879886,\n",
       " 0.008792324951856624,\n",
       " 0.0005736624025954798,\n",
       " -0.011575947553686836,\n",
       " -0.014604823711799228,\n",
       " -0.0010706832434757608,\n",
       " -0.016333859467509593,\n",
       " -0.03239793713600879,\n",
       " -0.016885677408007414,\n",
       " -0.03264319162361355,\n",
       " 0.012949365233692873,\n",
       " 0.022023731914972394,\n",
       " 0.020123019880583855,\n",
       " -0.0008698821794872632,\n",
       " 0.015119855341801493,\n",
       " -0.010619460639963735,\n",
       " -0.004163172109687656,\n",
       " -0.00033818106860168115,\n",
       " 0.007517008601231202,\n",
       " 0.009816256384009745,\n",
       " -0.016922466512470705,\n",
       " -0.0063520563043666335,\n",
       " 0.025015819899944072,\n",
       " -0.00232530618850187,\n",
       " 0.015647150627506573,\n",
       " -0.013537973209976564,\n",
       " -0.02443947464804062,\n",
       " 0.0090314466802874,\n",
       " -0.021545488458110842,\n",
       " -0.005098199488575984,\n",
       " -0.03264319162361355,\n",
       " -0.017560124454952773,\n",
       " -0.007277886872800427,\n",
       " 0.03117167214856561,\n",
       " -0.008246636976565054,\n",
       " 0.0007257959829267218,\n",
       " 0.0076641603624714805,\n",
       " -0.0030166134337321557,\n",
       " 0.007547664946520509,\n",
       " -0.024954505346720307,\n",
       " -0.014028479391218353,\n",
       " -0.00665249118903731,\n",
       " 0.006854825209988659,\n",
       " 0.021410597558605648,\n",
       " -0.0009442245385888284,\n",
       " -0.03293749700873926,\n",
       " 0.009853044557150458,\n",
       " -0.020785203271826397,\n",
       " 0.022600075304230693,\n",
       " 0.002185818513583918,\n",
       " 0.005144184472171232,\n",
       " -0.0015818827139015394,\n",
       " 0.007277886872800427,\n",
       " -0.019301422003720797,\n",
       " -0.0005430057662678681,\n",
       " -0.031367875738649416,\n",
       " -0.020466374300585365,\n",
       " -0.0014738179791696417,\n",
       " -0.009356406548057262,\n",
       " -0.02778717884607147,\n",
       " 0.023863129396137165,\n",
       " 0.0008177658919095571,\n",
       " -0.01744976086685321,\n",
       " -0.019154269311157938,\n",
       " 0.0013350967827331158,\n",
       " 0.01473971274865927,\n",
       " 0.004482001080928689,\n",
       " 0.007247230061849832,\n",
       " 0.0021260380814762244,\n",
       " 0.01343987141493466,\n",
       " -0.0029108480180292137,\n",
       " 0.012593748089311125,\n",
       " -0.027762655259956148,\n",
       " 0.013464396863695136,\n",
       " -0.011342957653107468,\n",
       " -0.01046004615434322,\n",
       " -0.004546380151094294,\n",
       " 0.00010480739036073582,\n",
       " 0.00806269704218406,\n",
       " -0.029332274667400838,\n",
       " -0.005876878295769499,\n",
       " -0.024807354516802604,\n",
       " 0.023998018432997207,\n",
       " -0.010361944359301314,\n",
       " -0.030239710683602986,\n",
       " -0.012195211875259834,\n",
       " -0.005128856299526579,\n",
       " -0.007167522819039574,\n",
       " 0.031515028431212275,\n",
       " -0.03840664041735778,\n",
       " -0.018626974956775438,\n",
       " 0.005984176435604649,\n",
       " 0.02077293961612358,\n",
       " -0.014077529357416727,\n",
       " 0.00901918395590716,\n",
       " 0.2468227454185966,\n",
       " -0.02309058241679506,\n",
       " 0.010625591536492566,\n",
       " 0.012360758188731757,\n",
       " 0.004555576961548829,\n",
       " 0.015916928701226653,\n",
       " 0.013550235934356803,\n",
       " -0.02504034534870455,\n",
       " 0.009411589273429622,\n",
       " 0.02660996475614924,\n",
       " 0.0006292275523012974,\n",
       " -0.008976264886237617,\n",
       " -0.020368272505543458,\n",
       " 0.006946794711517867,\n",
       " 0.003945509916091653,\n",
       " 0.004911194105930576,\n",
       " -0.015352846173703437,\n",
       " -0.0038780651648309887,\n",
       " -0.016971517409991657,\n",
       " -0.024083856572336294,\n",
       " -0.012385282706169657,\n",
       " 0.0022777884807744144,\n",
       " -0.014126580254937678,\n",
       " -0.006413369460606535,\n",
       " 0.010981208680874313,\n",
       " 0.02030695981496485,\n",
       " -0.030534014206083544,\n",
       " -0.0012638199883112876,\n",
       " -0.0034151498806140918,\n",
       " 0.010141217183102185,\n",
       " -0.002191949875774037,\n",
       " -0.010055378112440519,\n",
       " 0.017400709969332257,\n",
       " -0.020871041411165484,\n",
       " 0.010509097051864171,\n",
       " -0.01472745095560161,\n",
       " -0.005561114772792881,\n",
       " -0.01612539408436812,\n",
       " 0.005208563542336837,\n",
       " 0.014837814543701174,\n",
       " 0.014568035538658515,\n",
       " -0.006024030057009778,\n",
       " 0.014764238197419746,\n",
       " -0.016664950231808286,\n",
       " 0.0056101652046525435,\n",
       " 0.03144145022228569,\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c400cde4-0920-403f-b531-d5274f60dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d7dfe91-d2ae-4b3d-9e7a-4e5876171350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666694936097293"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1a16387-6223-4e71-81db-1bd942328221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7558295095982451"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding2, embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28a76b54-a270-49a7-b972-5b730b4d0e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7606652929197053"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding1, embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b04ba18-c1c7-45ec-bd9a-4b6515fa2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f37ee8f-339a-4ff3-bb9b-c77f5dd25801",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory=\"/home/tom/two/vstore/chroma/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5cca2687-6908-4576-bafe-194003f5582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f993b607-63ab-4f5b-bc09-b36412504990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "469ce86a-7038-4f56-82d5-90246fc4a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"is there an email I can ask for help?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6131c020-ae72-47a3-9342-118562dea47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectordb.similarity_search(question, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e20a402-6a3d-4998-a257-d378a9dc52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73817a96-507b-4d5b-8315-aded347c82a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cs229-qa@cs.stanford.edu. This goes to an acc ount that's read by all the TAs and me. So \\nrather than sending us email individually, if you send email to this account, it will \\nactually let us get back to you maximally quickly with answers to your questions.  \\nIf you're asking questions about homework probl ems, please say in the subject line which \\nassignment and which question the email refers to, since that will also help us to route \\nyour question to the appropriate TA or to me  appropriately and get the response back to \\nyou quickly.  \\nLet's see. Skipping ahead — let's see — for homework, one midterm, one open and term \\nproject. Notice on the honor code. So one thi ng that I think will help you to succeed and \\ndo well in this class and even help you to enjoy this cla ss more is if you form a study \\ngroup.  \\nSo start looking around where you' re sitting now or at the end of class today, mingle a \\nlittle bit and get to know your classmates. I strongly encourage you to form study groups \\nand sort of have a group of people to study with and have a group of your fellow students \\nto talk over these concepts with. You can also  post on the class news group if you want to \\nuse that to try to form a study group.  \\nBut some of the problems sets in this cla ss are reasonably difficult.  People that have \\ntaken the class before may tell you they were very difficult. And just I bet it would be \\nmore fun for you, and you'd probably have a be tter learning experience if you form a\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "515fb0ff-9265-4aeb-872b-6cb5191f1c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cs229-qa@cs.stanford.edu. This goes to an acc ount that's read by all the TAs and me. So \\nrather than sending us email individually, if you send email to this account, it will \\nactually let us get back to you maximally quickly with answers to your questions.  \\nIf you're asking questions about homework probl ems, please say in the subject line which \\nassignment and which question the email refers to, since that will also help us to route \\nyour question to the appropriate TA or to me  appropriately and get the response back to \\nyou quickly.  \\nLet's see. Skipping ahead — let's see — for homework, one midterm, one open and term \\nproject. Notice on the honor code. So one thi ng that I think will help you to succeed and \\ndo well in this class and even help you to enjoy this cla ss more is if you form a study \\ngroup.  \\nSo start looking around where you' re sitting now or at the end of class today, mingle a \\nlittle bit and get to know your classmates. I strongly encourage you to form study groups \\nand sort of have a group of people to study with and have a group of your fellow students \\nto talk over these concepts with. You can also  post on the class news group if you want to \\nuse that to try to form a study group.  \\nBut some of the problems sets in this cla ss are reasonably difficult.  People that have \\ntaken the class before may tell you they were very difficult. And just I bet it would be \\nmore fun for you, and you'd probably have a be tter learning experience if you form a\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6efffa06-cc9a-4862-899c-9d1f8d03266e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So all right, online resources. The class has a home page, so it's in on the handouts. I \\nwon't write on the chalkboard — http:// cs229.stanford.edu. And so when there are \\nhomework assignments or things like that, we  usually won't sort of — in the mission of \\nsaving trees, we will usually not give out many handouts in class. So homework \\nassignments, homework solutions will be posted online at the course home page.  \\nAs far as this class, I've also written, a nd I guess I've also revised every year a set of \\nfairly detailed lecture notes that cover the te chnical content of this  class. And so if you \\nvisit the course homepage, you'll also find the detailed lecture notes that go over in detail \\nall the math and equations and so on  that I'll be doing in class.  \\nThere's also a newsgroup, su.class.cs229, also written on the handout. This is a \\nnewsgroup that's sort of a forum for people in  the class to get to  know each other and \\nhave whatever discussions you want to ha ve amongst yourselves. So the class newsgroup \\nwill not be monitored by the TAs and me. But this is a place for you to form study groups \\nor find project partners or discuss homework problems and so on, and it's not monitored \\nby the TAs and me. So feel free to ta lk trash about this class there.  \\nIf you want to contact the teaching staff, pl ease use the email address written down here, \\ncs229-qa@cs.stanford.edu. This goes to an acc ount that's read by all the TAs and me. So\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9cde8ab0-191c-4111-b7c0-487b19a46755",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ceb6e57d-22b6-4bfb-a9c6-dfd7d9aca592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d8c75bb-374d-4cf5-8da4-409801ca010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question='what did they say about matlab?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae0b56ae-37c2-423e-aaf6-25562ddc699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectordb.similarity_search(question, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "212aa258-a412-4af4-be6b-74da209060d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your', metadata={'page': 8, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'}),\n",
       " Document(page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your', metadata={'page': 8, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'}),\n",
       " Document(page_content='into his office and he said, \"Oh, professo r, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s help ed me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"W ow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutori al in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical th ing is the discussion s ections. So discussion \\nsections will be taught by the TAs, and atte ndance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televi sed. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or th ree weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.', metadata={'page': 8, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'}),\n",
       " Document(page_content='into his office and he said, \"Oh, professo r, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s help ed me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"W ow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutori al in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical th ing is the discussion s ections. So discussion \\nsections will be taught by the TAs, and atte ndance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televi sed. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or th ree weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.', metadata={'page': 8, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'}),\n",
       " Document(page_content=\"same regardless of the group size, so with  a larger group, you probably — I recommend \\ntrying to form a team, but it's actually totally fine to do it in a sma ller group if you want.  \\nStudent : [Inaudible] what language [inaudible]?  \\nInstructor (Andrew Ng): So let's see. There is no C programming in this class other \\nthan any that you may choose to do yourself in your project. So all the homeworks can be \\ndone in MATLAB or Octave, and let's see. A nd I guess the program prerequisites is more \\nthe ability to understand big?O notation and know ledge of what a data structure, like a \\nlinked list or a queue or bina ry treatments, more so than  your knowledge of C or Java \\nspecifically. Yeah?  \\nStudent : Looking at the end semester project, I mean, what exactly will you be testing \\nover there? [Inaudible]?  \\nInstructor (Andrew Ng) : Of the project?  \\nStudent : Yeah.  \\nInstructor (Andrew Ng) : Yeah, let me answer that later.  In a couple of weeks, I shall \\ngive out a handout with guidelines for the pr oject. But for now, we should think of the \\ngoal as being to do a cool piec e of machine learning work that  will let you experience the\", metadata={'page': 9, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27a05ede-cceb-4f2a-b019-030c5cac38d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your', metadata={'page': 8, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1561430b-d0b9-448c-867d-74a30f940ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your', metadata={'page': 8, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d05d49b-3a75-45aa-a1b9-668c41df5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we should fix this edge case, so taht only distint documents are sent to the LLM for the final answer instead of sending dpulicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84abfab5-e487-49d4-a43c-94c6a7605f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"what did they say about regression in the third lecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5e440dab-6331-44da-b5b8-55a68696e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectordb.similarity_search(question, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b6d1948-c814-4967-9b6e-9d56bdfd824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4acaeaf5-cdf6-4ee5-ac89-23357fa4b4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture03.pdf'}\n",
      "{'page': 2, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture02.pdf'}\n",
      "{'page': 14, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture03.pdf'}\n",
      "{'page': 0, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture02.pdf'}\n",
      "{'page': 6, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture03.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for doc in results:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c8e5607-bd19-4985-81c5-1e4d1a6f93b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data sets as well. So don’t want to talk about  that. If you’re interested, look up the work \n",
      "of Andrew Moore on KD-trees. He, sort of, fi gured out ways to fit these models much \n",
      "more efficiently. That’s not something I want  to go into today. Okay? Let me move one. \n",
      "Let’s take more questions later.  \n",
      "So, okay. So that’s locally weighted regres sion. Remember the outline I had, I guess, at \n",
      "the beginning of this lecture. What I want to do now is talk about a probabilistic interpretation of linear regres sion, all right? And in partic ular of the – it’ll be this \n",
      "probabilistic interpretati on that let’s us move on to talk  about logistic regression, which \n",
      "will be our first classification algorithm. So le t’s put aside locally weighted regression for \n",
      "now. We’ll just talk about ordinary unwei ghted linear regression. Let’s ask the question \n",
      "of why least squares, right? Of all the thi ngs we could optimize how do we come up with \n",
      "this criteria for minimizing the square of  the area between the predictions of the \n",
      "hypotheses and the values Y predicted. So w hy not minimize the absolute value of the \n",
      "areas or the areas to the power of four or something? What I’m going to do now is \n",
      "present one set of assumptions that will serve to “justify” why we’re minimizing the sum \n",
      "of square zero. Okay?  \n",
      "It turns out that there are many assumptions th at are sufficient to justify why we do least \n",
      "squares and this is just one of them. So ju st because I present one set of assumptions\n"
     ]
    }
   ],
   "source": [
    "print(results[4].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6b4a01f2-b14c-49c6-9d62-5014e56e3d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Instructor (Andrew Ng) :All right, so who thought driving could be that dramatic, right? \\nSwitch back to the chalkboard, please. I s hould say, this work was done about 15 years \\nago and autonomous driving has come a long way. So many of you will have heard of the \\nDARPA Grand Challenge, where one of my colleagues, Sebastian Thrun, the winning \\nteam's drive a car across a desert by itself.  \\nSo Alvin was, I think, absolutely amazing wo rk for its time, but autonomous driving has \\nobviously come a long way since then. So what  you just saw was an example, again, of \\nsupervised learning, and in particular it was an  example of what they  call the regression \\nproblem, because the vehicle is trying to predict a continuous value variables of a \\ncontinuous value steering directions , we call the regression problem.  \\nAnd what I want to do today is talk about our first supervised learning algorithm, and it \\nwill also be to a regression task. So for the running example that I'm going to use \\nthroughout today's lecture, you're going to retu rn to the example of  trying to predict \\nhousing prices. So here's actually a data set collected by TA, Dan Ramage, on housing \\nprices in Portland, Oregon.  \\nSo here's a dataset of a number of houses of different sizes, and here are their asking \\nprices in thousands of dollars, $200,000. And so we  can take this data and plot it, square \\nfeet, best price, and so you make your other dataset like that. And the question is, given a\", metadata={'page': 2, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture02.pdf'})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#second entry\n",
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6e1a36f-9306-465c-bfd8-e39c395472c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"really makes a difference between a good so lution and amazing solution. And to give \\neveryone to just how we do points assignments, or what is it that causes a solution to get \\nfull marks, or just how to write amazing so lutions. Becoming a grad er is usually a good \\nway to do that.  \\nGraders are paid positions and you also get free  food, and it's usually fun for us to sort of \\nhang out for an evening and grade all the a ssignments. Okay, so I will send email. So \\ndon't email me yet if you want to be a grader. I'll send email to the entire class later with \\nthe administrative details and to solicit app lications. So you can email us back then, to \\napply, if you'd be interested in being a grader.  \\nOkay, any questions about that? All right, okay, so let's get started with today's material. \\nSo welcome back to the second lecture. What  I want to do today is talk about linear \\nregression, gradient descent, and the norma l equations. And I should also say, lecture \\nnotes have been posted online and so if some  of the math I go over today, I go over rather \\nquickly, if you want to see every equation wr itten out and work through the details more \\nslowly yourself, go to the course homepage and download detailed lecture notes that \\npretty much describe all the mathematical, te chnical contents I'm going to go over today.  \\nToday, I'm also going to delve into a fair amount  – some amount of linear algebra, and so\", metadata={'page': 0, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture02.pdf'})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1da8c32-398e-4fd3-ac76-447e526acb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"was there any discussion about PCA in second lecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "498bc06b-32a0-4cfc-ae3c-88dd2ffc2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectordb.similarity_search(question, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "252483cd-c8fd-461e-9ed4-950faff9b454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 8, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'}\n",
      "{'page': 8, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'}\n",
      "{'page': 0, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture02.pdf'}\n",
      "{'page': 8, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'}\n",
      "{'page': 8, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for doc in results:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4a4a7d2e-b7fb-466e-8190-184cbd9624b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'into his office and he said, \"Oh, professo r, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s help ed me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"W ow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutori al in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical th ing is the discussion s ections. So discussion \\nsections will be taught by the TAs, and atte ndance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televi sed. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or th ree weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0d5d15fa-cc34-45fc-8874-475b32b95ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'into his office and he said, \"Oh, professo r, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s help ed me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"W ow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutori al in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical th ing is the discussion s ections. So discussion \\nsections will be taught by the TAs, and atte ndance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televi sed. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or th ree weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1003b45-7766-4aa6-9c78-ed0790401cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today, I'm also going to delve into a fair amount  – some amount of linear algebra, and so \\nif you would like to see a refres her on linear algebra, this w eek's discussion section will \\nbe taught by the TAs and will be a refresher on linear algebra. So if some of the linear \\nalgebra I talk about today sort of seems to be going by pretty quickl y, or if you just want \\nto see some of the things I'm claiming today with our proof, if you wa nt to just see some \\nof those things written out in  detail, you can come to this week's discussion section.\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eca5aad8-e43c-42b1-aca7-025f78bd542a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2cd37f7-04b8-4819-9e9a-ea959e103a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_embedding = embeddings.embed_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "97bedd84-9985-43f2-92aa-ccf4db4709ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b856c749-95a9-4237-9a68-d375189f080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the embedding from vectordb\n",
    "# Get embeddings by document_id\n",
    "#db._collection.get(ids=['doc0', ..., 'docN'], include=['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a0535ffa-51ca-443c-8ae1-fb5a7a5b196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectordb._collection.get(ids=['doc110'], include=['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2921e086-6f29-4a61-9d79-8b39a62ac11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Today, I'm also going to delve into a fair amount  – some amount of linear algebra, and so \\nif you would like to see a refres her on linear algebra, this w eek's discussion section will \\nbe taught by the TAs and will be a refresher on linear algebra. So if some of the linear \\nalgebra I talk about today sort of seems to be going by pretty quickl y, or if you just want \\nto see some of the things I'm claiming today with our proof, if you wa nt to just see some \\nof those things written out in  detail, you can come to this week's discussion section.\", metadata={'page': 0, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture02.pdf'})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "974917f4-36fa-47fa-8064-43a9e72447d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\", metadata={'page': 8, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "506be321-ea9b-4aa7-a64d-6f8416bd8b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\", metadata={'page': 8, 'source': 'https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf'})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6b7b1-b7f4-499e-8f21-a96155362169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botenv",
   "language": "python",
   "name": "botenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
